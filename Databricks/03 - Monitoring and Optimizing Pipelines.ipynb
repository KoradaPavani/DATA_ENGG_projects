{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1ec526-be23-4be8-9a67-98a1e5b6f71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![databricks_academy_logo.png](../Includes/images/databricks_academy_logo.png \"databricks_academy_logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4222143a-7f7d-49e6-b97e-281212d0567b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Monitoring and Optimizing Pipelines\n",
    "\n",
    "You can monitor Lakeflow Spark Declarative Pipelines as they run and after a run to optimize, debug, and enrich your ETL logic.\n",
    "\n",
    "**Objective:** Use Databricks Lakeflow Spark Declarative Pipelines to monitor an ETL pipeline using the event log, and add data quality expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71f4a069-fdb5-48c7-8d82-68505d2c4728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Important: Select Environment 4\n",
    "The cells below may not work in other environments. To choose environment 4: \n",
    "1. Click the ![environment.png](../Includes/images/environment.png \"environment.png\") button on the right sidebar\n",
    "1. Open the **Environment version** dropdown\n",
    "1. Select **4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da363c0-4927-4e45-8646-2fc4b0283a0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Source Code and Data Quality Expectations\n",
    "The Lakeflow Pipelines UI provides an IDE experience for developing pipelines. We can view any source code file added to the pipeline and edit it in place. To do this:\n",
    "\n",
    "1. Click the name of the only source code file, **Pipeline - 1.py**, in the workspace pane on the left side of the page.\n",
    "\n",
    "This opens the file in the center pane of the page.\n",
    "\n",
    "2. Read through the comments on the source code page to get more information about pipeline syntax, data quality expectations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3aa5e57-f08b-40d3-9114-840f226a0c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Run the Pipeline\n",
    "\n",
    "Run the pipeline by clicking **Run pipeline** in the upper-right corner of the pipeline UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256a8e5f-4d54-45e4-8c58-04ca1d6d372e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitor a Pipeline While Running\n",
    "As soon as you have started the pipeline: \n",
    "1. In the left workspace pane, at the top of the pane, a counter has begun counting up the number of seconds that the pipeline run has taken. Click the time. This opens the pipeline's \"Runs\" page.\n",
    "2. If it is not already enabled, turn on **New pipeline monitoring** by clicking the clicking the dropdown at the top-left of the page and switching the feature on.\n",
    "\n",
    "This page allows you to monitor the pipeline run in real-time, or you can examine past runs of the pipeline using the dropdown at the upper-left of the page. We can open an event log in the bottom pane of this page:\n",
    "\n",
    "3. At the top of the bottom pane, on the right side, click ![issues-icon.png](../Includes/images/issues-icon.png \"issues-icon.png\")\n",
    "4. Then, click **View event log**.\n",
    "\n",
    "The event log shows you what is going on with the pipeline run from when it was initiated to when it is completed.\n",
    "\n",
    "5. Close the event log by clicking the \"X\" in the upper-right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f99f9dd0-b256-4ab6-8941-3a6a4388f1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## View Failed Expectations\n",
    "We setup three data quality expectations, and we intentionally have some data that fails two of these expectations. Let's look at how we can view the metrics related to our expectations.\n",
    "\n",
    "1. Wait for the pipeline to finish its run.\n",
    "\n",
    "Note the three datasets in the graph. These represent our bronze, silver, and gold data. The silver node has a couple of icons in the lower-right corner that show the number of warnings and the number of records dropped.\n",
    "\n",
    "2. Click one of the icons to open the **Expectations** pane.\n",
    "\n",
    "We see the two expectations that were violated: *valid_role* and *valid_country*, and there was one record each that violated the expectations. For *valid_role*, the record that violated the expectation was not written to the silver table. For *valid_country*, we are warned that there was a record that violated the expectation, but the record was still written to the table.\n",
    "\n",
    "3. Close the **Expectations** pane by clicking the \"X\" in the upper-right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35c1ac17-2caf-4a29-bb96-acd1969c8971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optimize a Pipeline\n",
    "You have the ability to check individual query metrics for each dataset in the pipeline. This can be useful to diagnose issues with long running queries or to simply improve query performance. Complete the following:\n",
    "\n",
    "1. Click **All tables** in the upper-right of the bottom pane.\n",
    "2. Click the **Performance** tab. \n",
    "\n",
    "Note the three queries.\n",
    "\n",
    "3. Click the query that refreshed the bronze table. \n",
    "\n",
    "Along with timing information, we get helpful metrics like the number of row read, the number of bytes read, and whether any data spilled onto disk. Although we don't have time to discuss these metrics in this course, we do have other courses on optimizing Spark that can be very helpful.\n",
    "\n",
    "4. Click **See query profile** at the bottom of the pane.\n",
    "\n",
    "We get a graph that displays how the query was executed. Read this from the bottom up.\n",
    "\n",
    "5. Click the bottom node, titled **Scan**.\n",
    "\n",
    "We get more information related to the scanning portion of the query where the data was actually read from the source. Among these are some very helpful metrics like the number of files read and the size of files read. Again, these metrics can help us see why queries might be taking longer than they need to be."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03 - Monitoring and Optimizing Pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}